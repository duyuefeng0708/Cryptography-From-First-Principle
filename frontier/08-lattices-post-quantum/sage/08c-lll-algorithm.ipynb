{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# The LLL Algorithm\n",
    "\n",
    "**Module 08c** | Lattices and Post-Quantum\n",
    "\n",
    "*The most important algorithm in lattice theory finds short vectors in polynomial time, but not short enough to break modern cryptography.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-motivation",
   "metadata": {},
   "source": [
    "> **Motivating Question:** Given a terrible basis with long, nearly-parallel vectors, can we algorithmically find a \"nice\" basis with short, nearly-orthogonal vectors? And can we do it in *polynomial time*?\n",
    "\n",
    "In notebook 08b we saw that finding the *shortest* vector in a lattice (SVP) is hard, believed to require exponential time in general. But what if we settle for an *approximately* short vector? In 1982, Arjen Lenstra, Hendrik Lenstra, and László Lovász discovered that a clever combination of Gram-Schmidt orthogonalization and a simple swap-and-reduce loop can transform *any* lattice basis into a \"reduced\" one with provably short vectors, all in polynomial time. Their algorithm, **LLL**, remains the single most important tool in computational lattice theory.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "1. Compute the Gram-Schmidt orthogonalization (GSO) of a lattice basis by hand and in SageMath\n",
    "2. State and check the two LLL conditions (size-reduction and the Lovász condition)\n",
    "3. Trace the LLL algorithm step-by-step on a small example\n",
    "4. Measure basis quality using orthogonality defect and Hermite factor\n",
    "5. Explain why LLL is powerful enough to break early lattice schemes but *not* modern ones like Kyber\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completion of [The Shortest Vector Problem](08b-shortest-vector-problem.ipynb)\n",
    "- Familiarity with lattices, bases, and the concept of SVP\n",
    "- Basic linear algebra: dot products, vector norms, projections\n",
    "\n",
    "> **Bridge from 08b:** We ended notebook 08b knowing that SVP is hard in the worst case. But \"hard in the worst case\" doesn't mean \"impossible to approximate.\" LLL gives us an efficient algorithm that *approximately* solves SVP, and that approximation is good enough to break many cryptographic schemes, though not all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-gso-intro",
   "metadata": {},
   "source": [
    "## 1. Gram-Schmidt Orthogonalization: The Geometric Backbone\n",
    "\n",
    "Before we can understand LLL, we need the **Gram-Schmidt orthogonalization (GSO)**. Given a basis $\\{\\mathbf{b}_1, \\mathbf{b}_2, \\ldots, \\mathbf{b}_n\\}$, GSO produces an *orthogonal* basis $\\{\\mathbf{b}_1^*, \\mathbf{b}_2^*, \\ldots, \\mathbf{b}_n^*\\}$ for the same vector space (but **not** the same lattice!) using the formulas:\n",
    "\n",
    "$$\\mathbf{b}_i^* = \\mathbf{b}_i - \\sum_{j=1}^{i-1} \\mu_{i,j} \\, \\mathbf{b}_j^*, \\qquad \\text{where } \\mu_{i,j} = \\frac{\\langle \\mathbf{b}_i, \\mathbf{b}_j^* \\rangle}{\\langle \\mathbf{b}_j^*, \\mathbf{b}_j^* \\rangle}$$\n",
    "\n",
    "The coefficients $\\mu_{i,j}$ measure how much of $\\mathbf{b}_j^*$ we need to subtract from $\\mathbf{b}_i$ to make it orthogonal to all previous vectors.\n",
    "\n",
    "**Key insight:** The GSO vectors $\\mathbf{b}_i^*$ are the \"orthogonal components\" of the basis. Their norms $\\|\\mathbf{b}_i^*\\|$ tell us how much \"new\" length each basis vector contributes in a direction orthogonal to all previous vectors.\n",
    "\n",
    "Let's start with the simplest case: two vectors in $\\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-gso-2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gram-Schmidt in 2D, by hand, then verified with SageMath\n",
    "b1 = vector(QQ, [3, 1])\n",
    "b2 = vector(QQ, [2, 3])\n",
    "\n",
    "# Step 1: b1* = b1 (first vector is unchanged)\n",
    "b1_star = b1\n",
    "\n",
    "# Step 2: compute mu_{2,1} and subtract the projection\n",
    "mu_21 = b2.dot_product(b1_star) / b1_star.dot_product(b1_star)\n",
    "b2_star = b2 - mu_21 * b1_star\n",
    "\n",
    "print(f'b1       = {b1}')\n",
    "print(f'b2       = {b2}')\n",
    "print(f'mu_21    = {mu_21} = {float(mu_21):.4f}')\n",
    "print(f'b1*      = {b1_star}')\n",
    "print(f'b2*      = {b2_star}')\n",
    "print(f'\\nVerification: b1* . b2* = {b1_star.dot_product(b2_star)}  (should be 0)')\n",
    "\n",
    "# SageMath has a built-in Gram-Schmidt method\n",
    "B = matrix(QQ, [b1, b2])\n",
    "G, mu_matrix = B.gram_schmidt()\n",
    "print(f'\\nSageMath GSO basis:\\n{G}')\n",
    "print(f'Mu coefficients:\\n{mu_matrix}')\n",
    "print(f'Our manual b2* matches SageMath: {b2_star == G[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-gso-3d-header",
   "metadata": {},
   "source": [
    "### GSO in 3D\n",
    "\n",
    "Now let's see the pattern with three vectors. Each new vector gets projected onto *all* previous GSO vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-gso-3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSO for a 3D basis\n",
    "B3 = matrix(QQ, [[1, 1, 1],\n",
    "                  [-1, 0, 2],\n",
    "                  [3, 5, 6]])\n",
    "\n",
    "G3, mu3 = B3.gram_schmidt()\n",
    "\n",
    "print('Original basis:')\n",
    "for i in range(3):\n",
    "    print(f'  b{i+1}  = {B3[i]}  (norm = {float(B3[i].norm()):.4f})')\n",
    "\n",
    "print('\\nGSO basis:')\n",
    "for i in range(3):\n",
    "    print(f'  b{i+1}* = {G3[i]}  (norm = {float(G3[i].norm()):.4f})')\n",
    "\n",
    "print(f'\\nMu matrix:\\n{mu3}')\n",
    "\n",
    "# Verify orthogonality\n",
    "print(f'\\nb1*.b2* = {G3[0].dot_product(G3[1])}  (should be 0)')\n",
    "print(f'b1*.b3* = {G3[0].dot_product(G3[2])}  (should be 0)')\n",
    "print(f'b2*.b3* = {G3[1].dot_product(G3[2])}  (should be 0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-lll-conditions",
   "metadata": {},
   "source": [
    "> **Important:** The GSO vectors $\\mathbf{b}_i^*$ are *not* lattice vectors in general (they have rational coordinates). GSO tells us about the *geometry* of the basis, but we cannot simply use the GSO basis as our lattice basis. LLL works with the *original integer basis* while using the GSO vectors as a guide.\n",
    "\n",
    "## 2. The LLL Conditions\n",
    "\n",
    "A basis $\\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\}$ is **LLL-reduced** (with parameter $\\delta$, typically $\\delta = 3/4$) if two conditions hold:\n",
    "\n",
    "### Condition 1: Size-Reduced\n",
    "$$|\\mu_{i,j}| \\le \\frac{1}{2} \\quad \\text{for all } 1 \\le j < i \\le n$$\n",
    "\n",
    "This means each basis vector has been \"cleaned up\" — its projections onto earlier GSO vectors are at most half. If $|\\mu_{i,j}| > 1/2$, we can *size-reduce* by replacing $\\mathbf{b}_i \\leftarrow \\mathbf{b}_i - \\lceil \\mu_{i,j} \\rfloor \\, \\mathbf{b}_j$ (where $\\lceil \\cdot \\rfloor$ is rounding to the nearest integer).\n",
    "\n",
    "### Condition 2: Lovász Condition\n",
    "$$\\|\\mathbf{b}_i^*\\|^2 \\ge \\left(\\delta - \\mu_{i,i-1}^2\\right) \\|\\mathbf{b}_{i-1}^*\\|^2 \\quad \\text{for all } 2 \\le i \\le n$$\n",
    "\n",
    "This prevents the GSO norms from *decreasing too quickly*. If the condition fails for index $i$, we **swap** $\\mathbf{b}_i$ and $\\mathbf{b}_{i-1}$.\n",
    "\n",
    "The standard choice $\\delta = 3/4$ gives a good balance between reduction quality and speed. Increasing $\\delta$ toward 1 gives better reduction but slower convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-check-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lll_conditions(B, delta=3/4):\n",
    "    \"\"\"\n",
    "    Check whether basis B satisfies the LLL conditions.\n",
    "    Returns a detailed report.\n",
    "    \"\"\"\n",
    "    B = matrix(QQ, B)\n",
    "    n = B.nrows()\n",
    "    G, mu = B.gram_schmidt()\n",
    "    \n",
    "    print('=== LLL Condition Check ===')\n",
    "    print(f'Basis ({n} vectors), delta = {delta}\\n')\n",
    "    \n",
    "    # Check size-reduction\n",
    "    size_reduced = True\n",
    "    print('Condition 1 (Size-Reduced): |mu_{i,j}| <= 1/2')\n",
    "    for i in range(1, n):\n",
    "        for j in range(i):\n",
    "            mu_val = mu[i][j]\n",
    "            ok = abs(mu_val) <= 1/2\n",
    "            status = 'OK' if ok else 'FAIL'\n",
    "            print(f'  mu_{{{i+1},{j+1}}} = {float(mu_val):+.4f}  [{status}]')\n",
    "            if not ok:\n",
    "                size_reduced = False\n",
    "    \n",
    "    # Check Lovasz condition\n",
    "    lovasz_ok = True\n",
    "    print(f'\\nCondition 2 (Lovasz): ||b*_i||^2 >= (delta - mu^2_{{i,i-1}}) * ||b*_{{i-1}}||^2')\n",
    "    for i in range(1, n):\n",
    "        norm_i_sq = G[i].dot_product(G[i])\n",
    "        norm_im1_sq = G[i-1].dot_product(G[i-1])\n",
    "        mu_val = mu[i][i-1]\n",
    "        threshold = (delta - mu_val^2) * norm_im1_sq\n",
    "        ok = norm_i_sq >= threshold\n",
    "        status = 'OK' if ok else 'FAIL'\n",
    "        print(f'  i={i+1}: ||b*_{i+1}||^2 = {float(norm_i_sq):.4f} '\n",
    "              f'>= ({delta} - {float(mu_val^2):.4f}) * {float(norm_im1_sq):.4f} '\n",
    "              f'= {float(threshold):.4f}  [{status}]')\n",
    "        if not ok:\n",
    "            lovasz_ok = False\n",
    "    \n",
    "    print(f'\\nLLL-reduced? {size_reduced and lovasz_ok}')\n",
    "    return size_reduced and lovasz_ok\n",
    "\n",
    "# Test on a non-reduced basis\n",
    "B_bad = matrix(ZZ, [[201, 37], [1648, 297]])\n",
    "print('--- Bad basis ---')\n",
    "check_lll_conditions(B_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-lll-stepbystep-header",
   "metadata": {},
   "source": [
    "> **Checkpoint:** Look at the output above. Which condition failed — size-reduction, Lovász, or both? What does the failure of each condition tell you geometrically about the basis?\n",
    "\n",
    "## 3. Step-by-Step LLL on a 2D Example\n",
    "\n",
    "Let's trace every single step of the LLL algorithm on a small example. This is the best way to build intuition.\n",
    "\n",
    "**Input basis:**\n",
    "$$\\mathbf{b}_1 = (201, 37), \\quad \\mathbf{b}_2 = (1648, 297)$$\n",
    "\n",
    "This is a \"bad\" basis: the vectors are long and nearly parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-lll-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lll_step_by_step(B, delta=QQ(3)/QQ(4)):\n",
    "    \"\"\"\n",
    "    LLL algorithm with verbose output showing every step.\n",
    "    Works over QQ for exact arithmetic.\n",
    "    \"\"\"\n",
    "    B = matrix(QQ, B)\n",
    "    n = B.nrows()\n",
    "    step = 0\n",
    "    \n",
    "    print(f'=== LLL Algorithm (delta={delta}) ===')\n",
    "    print(f'Input basis:')\n",
    "    for i in range(n):\n",
    "        print(f'  b{i+1} = {B[i]}  (norm = {float(B[i].norm()):.2f})')\n",
    "    print()\n",
    "    \n",
    "    k = 1\n",
    "    while k < n:\n",
    "        step += 1\n",
    "        G, mu = B.gram_schmidt()\n",
    "        \n",
    "        print(f'--- Step {step}: processing index k={k+1} ---')\n",
    "        \n",
    "        # Size-reduce b_k against all previous vectors\n",
    "        for j in range(k-1, -1, -1):\n",
    "            mu_val = mu[k][j]\n",
    "            if abs(mu_val) > QQ(1)/QQ(2):\n",
    "                r = mu_val.round()\n",
    "                print(f'  Size-reduce: mu_{{{k+1},{j+1}}} = {float(mu_val):.4f}, '\n",
    "                      f'round = {r}')\n",
    "                print(f'    b{k+1} <- b{k+1} - {r}*b{j+1} = {B[k]} - {r}*{B[j]}')\n",
    "                B[k] = B[k] - r * B[j]\n",
    "                print(f'    b{k+1} = {B[k]}  (norm = {float(B[k].norm()):.2f})')\n",
    "                G, mu = B.gram_schmidt()\n",
    "        \n",
    "        # Check Lovasz condition\n",
    "        norm_k_sq = G[k].dot_product(G[k])\n",
    "        norm_km1_sq = G[k-1].dot_product(G[k-1])\n",
    "        mu_val = mu[k][k-1]\n",
    "        lovasz_threshold = (delta - mu_val^2) * norm_km1_sq\n",
    "        \n",
    "        print(f'  Lovasz check: ||b*_{k+1}||^2 = {float(norm_k_sq):.4f} '\n",
    "              f'>= {float(lovasz_threshold):.4f}?')\n",
    "        \n",
    "        if norm_k_sq >= lovasz_threshold:\n",
    "            print(f'  Lovasz PASSES -> move to k={k+2}')\n",
    "            k += 1\n",
    "        else:\n",
    "            print(f'  Lovasz FAILS -> SWAP b{k} and b{k+1}, back to k={k}')\n",
    "            B.swap_rows(k-1, k)\n",
    "            k = max(k-1, 1)\n",
    "        \n",
    "        print(f'  Current basis:')\n",
    "        for i in range(n):\n",
    "            print(f'    b{i+1} = {B[i]}  (norm = {float(B[i].norm()):.2f})')\n",
    "        print()\n",
    "    \n",
    "    print(f'=== Done in {step} steps ===')\n",
    "    print(f'Output basis:')\n",
    "    for i in range(n):\n",
    "        print(f'  b{i+1} = {B[i]}  (norm = {float(B[i].norm()):.2f})')\n",
    "    return matrix(ZZ, B)\n",
    "\n",
    "# Run on our bad basis\n",
    "B_bad = matrix(ZZ, [[201, 37], [1648, 297]])\n",
    "B_reduced = lll_step_by_step(B_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-verify-reduced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the result is LLL-reduced and matches SageMath\n",
    "print('=== Verification ===')\n",
    "print(f'Output basis:\\n{B_reduced}\\n')\n",
    "check_lll_conditions(B_reduced)\n",
    "\n",
    "B_sage = matrix(ZZ, [[201, 37], [1648, 297]]).LLL()\n",
    "print(f'\\nSageMath LLL gives:\\n{B_sage}')\n",
    "print(f'Same result (up to signs)? {B_reduced == B_sage or B_reduced == -B_sage}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sage-lll-header",
   "metadata": {},
   "source": [
    "> **Checkpoint:** In the step-by-step trace above, find the swap step. Before the swap, which vector was longer — $\\mathbf{b}_1$ or $\\mathbf{b}_2$? After the swap and re-reduction, are the vectors more nearly orthogonal? (Hint: compute the angle between them.)\n",
    "\n",
    "## 4. SageMath's Built-in LLL\n",
    "\n",
    "In practice, you will use `M.LLL()` on an integer matrix. Let's see the dramatic improvement it produces on larger, more badly-conditioned bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-sage-lll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A badly-conditioned 3D basis\n",
    "B = matrix(ZZ, [[15, 23, 11],\n",
    "                 [46, 79, 31],\n",
    "                 [32, 48, 97]])\n",
    "\n",
    "L = B.LLL()\n",
    "\n",
    "print('BEFORE LLL:')\n",
    "for i in range(3):\n",
    "    print(f'  b{i+1} = {list(B[i]):>20s}  norm = {float(B[i].norm()):8.2f}')\n",
    "\n",
    "print('\\nAFTER LLL:')\n",
    "for i in range(3):\n",
    "    print(f'  b{i+1} = {list(L[i]):>20s}  norm = {float(L[i].norm()):8.2f}')\n",
    "\n",
    "# Check that both bases span the same lattice (transition matrix has det +/- 1)\n",
    "T = B.solve_left(L)\n",
    "print(f'\\nTransition matrix determinant: {T.det()} (must be +/- 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-sage-lll-bigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A larger example: random 6D lattice worsened by a unimodular transform\n",
    "set_random_seed(42)\n",
    "n = 6\n",
    "B_random = random_matrix(ZZ, n, n, x=-100, y=100)\n",
    "\n",
    "# Upper-triangular unimodular matrix (det = 1) to scramble the basis\n",
    "U_bad = matrix(ZZ, [[1,3,2,0,-1,4],\n",
    "                     [0,1,0,2,1,-3],\n",
    "                     [0,0,1,1,0,2],\n",
    "                     [0,0,0,1,0,1],\n",
    "                     [0,0,0,0,1,0],\n",
    "                     [0,0,0,0,0,1]])\n",
    "B_bad = U_bad * B_random\n",
    "L_good = B_bad.LLL()\n",
    "\n",
    "print(f'Dimension: {n}')\n",
    "print(f'\\nBefore LLL, vector norms:')\n",
    "for i in range(n):\n",
    "    print(f'  ||b{i+1}|| = {float(B_bad[i].norm()):.2f}')\n",
    "\n",
    "print(f'\\nAfter LLL, vector norms:')\n",
    "for i in range(n):\n",
    "    print(f'  ||b{i+1}|| = {float(L_good[i].norm()):.2f}')\n",
    "\n",
    "print(f'\\nShortest vector norm before: {float(min(B_bad[i].norm() for i in range(n))):.2f}')\n",
    "print(f'Shortest vector norm after:  {float(L_good[0].norm()):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-quality-header",
   "metadata": {},
   "source": [
    "## 5. Quality Metrics: How Good Is Our Reduced Basis?\n",
    "\n",
    "We need quantitative ways to measure how \"nice\" a basis is. Two key metrics:\n",
    "\n",
    "### Orthogonality Defect\n",
    "$$\\text{od}(B) = \\frac{\\prod_{i=1}^n \\|\\mathbf{b}_i\\|}{|\\det(B)|}$$\n",
    "\n",
    "By Hadamard's inequality, $\\text{od}(B) \\ge 1$, with equality *only* when the basis is perfectly orthogonal. The closer to 1, the better.\n",
    "\n",
    "### Hermite Factor\n",
    "$$\\gamma = \\frac{\\|\\mathbf{b}_1\\|}{\\det(L)^{1/n}}$$\n",
    "\n",
    "This measures how short the first (shortest) vector is relative to the lattice determinant. LLL guarantees $\\gamma \\le 2^{(n-1)/4}$. Smaller is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-quality-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basis_quality(B, label=''):\n",
    "    \"\"\"\n",
    "    Compute and display quality metrics for a lattice basis.\n",
    "    \"\"\"\n",
    "    B = matrix(QQ, B)\n",
    "    n = B.nrows()\n",
    "    \n",
    "    # Orthogonality defect\n",
    "    norms_product = prod(B[i].norm() for i in range(n))\n",
    "    det_abs = abs(B.det())\n",
    "    orth_defect = float(norms_product / det_abs)\n",
    "    \n",
    "    # Hermite factor\n",
    "    b1_norm = float(B[0].norm())\n",
    "    det_root = float(det_abs^(1/n))\n",
    "    hermite = b1_norm / det_root\n",
    "    \n",
    "    if label:\n",
    "        print(f'--- {label} ---')\n",
    "    print(f'  ||b_1||              = {b1_norm:.4f}')\n",
    "    print(f'  |det(B)|             = {float(det_abs):.4f}')\n",
    "    print(f'  Orthogonality defect = {orth_defect:.4f}  (1.0 = perfect)')\n",
    "    print(f'  Hermite factor       = {hermite:.4f}')\n",
    "    return orth_defect, hermite\n",
    "\n",
    "# Compare before and after LLL\n",
    "B = matrix(ZZ, [[201, 37], [1648, 297]])\n",
    "L = B.LLL()\n",
    "\n",
    "basis_quality(B, 'Before LLL')\n",
    "print()\n",
    "basis_quality(L, 'After LLL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-misconception-1",
   "metadata": {},
   "source": [
    "> **Common mistake:** \"LLL finds the shortest vector.\" **No!** LLL finds an *approximately* shortest vector. The first vector $\\mathbf{b}_1$ of the LLL-reduced basis satisfies:\n",
    ">\n",
    "> $$\\|\\mathbf{b}_1\\| \\le 2^{(n-1)/2} \\cdot \\lambda_1(L)$$\n",
    ">\n",
    "> where $\\lambda_1(L)$ is the true shortest vector length. This approximation factor $2^{(n-1)/2}$ grows **exponentially** with dimension. In 2D it's harmless ($\\sqrt{2} \\approx 1.41$), but in 500D it's astronomically large ($2^{249.5}$). LLL does *not* solve SVP — it solves an exponential approximation of SVP, and it does so in polynomial time.\n",
    "\n",
    "## 6. LLL Guarantees and Approximation Factor\n",
    "\n",
    "Let's make the guarantees concrete. The LLL algorithm (with $\\delta = 3/4$) guarantees:\n",
    "\n",
    "1. **Running time:** Polynomial in $n$ and the bit-length of the input — specifically $O(n^5 d \\log^3 B)$ where $d$ is the dimension and $B$ bounds the entries.\n",
    "\n",
    "2. **Output quality:** The first vector satisfies $\\|\\mathbf{b}_1\\| \\le 2^{(n-1)/2} \\cdot \\lambda_1(L)$.\n",
    "\n",
    "3. **All vectors bounded:** For each $i$, $\\|\\mathbf{b}_i\\| \\le 2^{(n-1)/2} \\cdot \\lambda_i(L)$ where $\\lambda_i$ is the $i$-th successive minimum.\n",
    "\n",
    "Let's visualize how the approximation factor grows with dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-approx-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximation factor 2^((n-1)/2) as a function of dimension\n",
    "print('LLL approximation factor 2^((n-1)/2):')\n",
    "print(f'{\"Dim\":>5s}  {\"Factor\":>15s}  {\"log10(Factor)\":>15s}')\n",
    "print('-' * 40)\n",
    "for n in [2, 5, 10, 20, 50, 100, 200, 500]:\n",
    "    factor = 2^((n-1)/2)\n",
    "    log_f = float((n-1)/2 * log(2.0, 10))\n",
    "    if n <= 50:\n",
    "        print(f'{n:5d}  {float(factor):15.1f}  {log_f:15.2f}')\n",
    "    else:\n",
    "        print(f'{n:5d}  {\"2^\" + str((n-1)/2):>15s}  {log_f:15.2f}')\n",
    "\n",
    "# Plot the log of the approximation factor\n",
    "p = list_plot([(n, float((n-1)/2)) for n in range(2, 101)],\n",
    "              plotjoined=True, axes_labels=['Dimension $n$', '$\\\\log_2$ approx factor'],\n",
    "              title='LLL Approximation Factor Growth', color='red', thickness=2)\n",
    "show(p, figsize=(8, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-applications-header",
   "metadata": {},
   "source": [
    "The approximation factor grows *linearly* on a log scale, meaning it's *exponential* in the dimension. This is the fundamental limitation of LLL:\n",
    "\n",
    "- In low dimensions (say $n \\le 40$), LLL often finds vectors **much** shorter than the worst-case guarantee — sometimes close to the actual shortest vector.\n",
    "- In high dimensions (say $n \\ge 200$), the approximation factor becomes so large that LLL-reduced vectors can be far from optimal.\n",
    "\n",
    "This gap is precisely what modern lattice-based cryptography exploits.\n",
    "\n",
    "## 7. Applications: Breaking Things with LLL\n",
    "\n",
    "LLL is an incredibly versatile tool. Here we demonstrate two classic applications.\n",
    "\n",
    "### Application 1: Breaking Low-Dimensional Knapsack Cryptography\n",
    "\n",
    "The **subset-sum (knapsack) problem** was once proposed as a basis for public-key encryption (Merkle-Hellman, 1978). The idea: public key is a set of weights $\\{a_1, \\ldots, a_n\\}$ and a target sum $s = \\sum_{i \\in S} a_i$. Finding the subset $S$ is the hard problem.\n",
    "\n",
    "LLL can break this by embedding it as a lattice problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-knapsack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking a toy knapsack instance with LLL\n",
    "set_random_seed(123)\n",
    "n = 8  # dimension (number of weights)\n",
    "a = [randint(1, 2^20) for _ in range(n)]  # public weights\n",
    "\n",
    "# Secret binary message\n",
    "x_secret = vector(ZZ, [1, 0, 1, 1, 0, 0, 1, 0])\n",
    "s = sum(a[i] * x_secret[i] for i in range(n))  # target sum\n",
    "\n",
    "print(f'Public weights: {a}')\n",
    "print(f'Target sum:     {s}')\n",
    "print(f'Secret message: {x_secret}')\n",
    "\n",
    "# Build the lattice for knapsack:\n",
    "#   [ I_n  |  0 ]\n",
    "#   [ a^T  | -s ]\n",
    "M = matrix(ZZ, n+1, n+1)\n",
    "for i in range(n):\n",
    "    M[i, i] = 1          # identity block\n",
    "    M[n, i] = a[i]       # weights in last row\n",
    "M[n, n] = -s              # target sum (negated)\n",
    "\n",
    "print(f'\\nLattice matrix ({n+1}x{n+1}):')\n",
    "print(M)\n",
    "\n",
    "# Apply LLL\n",
    "L = M.LLL()\n",
    "print(f'\\nLLL-reduced basis:')\n",
    "print(L)\n",
    "\n",
    "# Look for a row with entries in {0,1} and last entry 0\n",
    "print(f'\\nSearching for solution...')\n",
    "for sign in [1, -1]:\n",
    "    for i in range(n+1):\n",
    "        row = sign * L[i]\n",
    "        candidate = row[:n]\n",
    "        if row[n] == 0 and all(c in [0, 1] for c in candidate):\n",
    "            print(f'  Found in row {i}{\" (negated)\" if sign == -1 else \"\"}: {candidate}')\n",
    "            print(f'  Matches secret?  {candidate == x_secret}')\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-knapsack-explain",
   "metadata": {},
   "source": [
    "LLL found the secret binary message by reducing the lattice! The key idea: the solution vector $(x_1, \\ldots, x_n, 0)$ is a *short* vector in the constructed lattice (its entries are just 0s and 1s), so LLL can find it.\n",
    "\n",
    "This is essentially how Shamir broke the Merkle-Hellman knapsack cryptosystem in 1982 — the same year LLL was published.\n",
    "\n",
    "### Application 2: Finding Integer Relations\n",
    "\n",
    "Given real numbers $\\alpha_1, \\ldots, \\alpha_n$, an **integer relation** is a vector $(m_1, \\ldots, m_n) \\in \\mathbb{Z}^n \\setminus \\{0\\}$ such that $\\sum m_i \\alpha_i = 0$. LLL can find these! Classic application: recover minimal polynomials of algebraic numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-intrelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the minimal polynomial of alpha = 2^(1/3) via LLL\n",
    "# We know alpha^3 - 2 = 0, so the relation (coeff vector) should be (-2, 0, 0, 1)\n",
    "\n",
    "alpha = RR(2^(1/3))\n",
    "precision = 10^12  # scaling factor for real -> integer\n",
    "n = 4  # we look for a degree-3 relation: c0 + c1*x + c2*x^2 + c3*x^3 = 0\n",
    "\n",
    "# Construct the HJLS-type lattice\n",
    "M_ext = matrix(ZZ, n, n+1)\n",
    "for i in range(n):\n",
    "    M_ext[i, i] = 1\n",
    "    M_ext[i, n] = round(precision * alpha^i)\n",
    "\n",
    "L = M_ext.LLL()\n",
    "\n",
    "print(f'Looking for minimal polynomial of 2^(1/3)...')\n",
    "print(f'\\nLLL-reduced basis (first n columns = coefficients):')\n",
    "for i in range(n):\n",
    "    coeffs = L[i][:n]\n",
    "    residual = L[i][n]\n",
    "    print(f'  Row {i}: coeffs = {coeffs}, residual = {residual}')\n",
    "\n",
    "# The row with smallest residual gives our relation\n",
    "best_row = min(range(n), key=lambda i: abs(L[i][n]))\n",
    "coeffs = L[best_row][:n]\n",
    "print(f'\\nBest relation: {coeffs[0]} + {coeffs[1]}*x + {coeffs[2]}*x^2 + {coeffs[3]}*x^3 = 0')\n",
    "R.<x> = QQ[]\n",
    "p = sum(coeffs[i] * x^i for i in range(n))\n",
    "print(f'As polynomial: {p}')\n",
    "print(f'Factors: {p.factor()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-foreshadow",
   "metadata": {},
   "source": [
    "> **Crypto Foreshadowing:** LLL breaks many early lattice-based schemes (knapsack crypto, certain NTRU parameters, low-exponent RSA attacks) but **NOT** modern ones like Kyber/ML-KEM. Why? Modern schemes are designed in dimensions $n \\ge 256$ with carefully chosen parameters so that the best known lattice algorithms (even ones better than LLL, like BKZ) cannot find short enough vectors in reasonable time. In notebook 08f, we'll see exactly how Kyber's parameter selection defeats lattice reduction.\n",
    "\n",
    "## 8. Limitations of LLL\n",
    "\n",
    "Let's see empirically how LLL's quality degrades with dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-limitations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure LLL's Hermite factor across dimensions\n",
    "set_random_seed(0)\n",
    "\n",
    "dimensions = [5, 10, 20, 30, 40, 50, 60, 70, 80]\n",
    "hermite_factors = []\n",
    "\n",
    "print(f'{\"Dim\":>5s}  {\"||b1|| before\":>15s}  {\"||b1|| after\":>15s}  '\n",
    "      f'{\"Hermite factor\":>15s}  {\"2^((n-1)/4)\":>12s}')\n",
    "print('-' * 75)\n",
    "\n",
    "for n in dimensions:\n",
    "    B = random_matrix(ZZ, n, n, x=-99, y=100)\n",
    "    while B.det() == 0:\n",
    "        B = random_matrix(ZZ, n, n, x=-99, y=100)\n",
    "    \n",
    "    b1_before = float(B[0].norm())\n",
    "    L = B.LLL()\n",
    "    b1_after = float(L[0].norm())\n",
    "    det_val = abs(B.det())\n",
    "    det_root = float(det_val^(QQ(1)/QQ(n)))\n",
    "    hermite = b1_after / det_root\n",
    "    hermite_factors.append((n, hermite))\n",
    "    theoretical_bound = float(2^((n-1)/4))\n",
    "    \n",
    "    print(f'{n:5d}  {b1_before:15.2f}  {b1_after:15.2f}  '\n",
    "          f'{hermite:15.4f}  {theoretical_bound:12.2f}')\n",
    "\n",
    "# Plot\n",
    "p1 = list_plot(hermite_factors, plotjoined=True, color='blue',\n",
    "               legend_label='Actual Hermite factor', thickness=2)\n",
    "p2 = list_plot([(n, float(2^((n-1)/4))) for n in range(5, 81)],\n",
    "               plotjoined=True, color='red', linestyle='dashed',\n",
    "               legend_label='Theoretical bound $2^{(n-1)/4}$', thickness=2)\n",
    "show(p1 + p2, axes_labels=['Dimension $n$', 'Hermite factor'],\n",
    "     title='LLL Quality vs Dimension', figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-limitations-explain",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "\n",
    "1. The actual Hermite factor is well below the theoretical bound (LLL performs better in practice than worst-case theory suggests).\n",
    "2. Nevertheless, the Hermite factor grows with dimension — LLL produces increasingly \"loose\" approximations as $n$ increases.\n",
    "3. For cryptographic applications like Kyber ($n \\ge 256$), even running LLL (or its more powerful variant BKZ) cannot find vectors short enough to break the scheme.\n",
    "\n",
    "This is the core insight of modern lattice-based cryptography: **lattice reduction algorithms exist and are powerful, but their approximation quality degrades fast enough that we can design secure schemes by choosing dimensions and parameters carefully.**\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1 (Worked)\n",
    "\n",
    "**Problem:** Apply LLL to the basis $B = \\begin{pmatrix} 1 & 1 \\\\ 0 & 17 \\end{pmatrix}$. Trace each step by hand, then verify with SageMath. Compute the Hermite factor before and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-exercise-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1. Worked Solution\n",
    "\n",
    "B = matrix(ZZ, [[1, 1], [0, 17]])\n",
    "print('Input basis:')\n",
    "print(B)\n",
    "print(f'  ||b1|| = {float(B[0].norm()):.4f}')\n",
    "print(f'  ||b2|| = {float(B[1].norm()):.4f}')\n",
    "\n",
    "# Step 1: Compute GSO\n",
    "G, mu = B.gram_schmidt()\n",
    "print(f'\\nGSO:')\n",
    "print(f'  b1* = {G[0]}, ||b1*|| = {float(G[0].norm()):.4f}')\n",
    "print(f'  b2* = {G[1]}, ||b2*|| = {float(G[1].norm()):.4f}')\n",
    "print(f'  mu_21 = {mu[1][0]} = {float(mu[1][0]):.4f}')\n",
    "\n",
    "# Step 2: Size-reduce check\n",
    "print(f'\\n|mu_21| = {abs(mu[1][0])} <= 1/2? {abs(mu[1][0]) <= QQ(1)/QQ(2)}')\n",
    "\n",
    "# Step 3: Check Lovasz condition\n",
    "delta = QQ(3)/QQ(4)\n",
    "norm_b2_star_sq = G[1].dot_product(G[1])\n",
    "norm_b1_star_sq = G[0].dot_product(G[0])\n",
    "threshold = (delta - mu[1][0]^2) * norm_b1_star_sq\n",
    "print(f'\\nLovasz: ||b2*||^2 = {norm_b2_star_sq} = {float(norm_b2_star_sq):.4f}')\n",
    "print(f'  (delta - mu^2) * ||b1*||^2 = ({delta} - {mu[1][0]^2}) * {norm_b1_star_sq}')\n",
    "print(f'  = {threshold} = {float(threshold):.4f}')\n",
    "print(f'  Lovasz holds? {norm_b2_star_sq >= threshold}')\n",
    "\n",
    "# Trace full algorithm and verify\n",
    "print('\\n--- Full LLL trace ---')\n",
    "B_reduced = lll_step_by_step(B)\n",
    "\n",
    "print('\\n--- Verification ---')\n",
    "L = B.LLL()\n",
    "print(f'SageMath LLL:\\n{L}')\n",
    "\n",
    "print('\\n--- Quality comparison ---')\n",
    "basis_quality(B, 'Before LLL')\n",
    "print()\n",
    "basis_quality(L, 'After LLL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-exercise-2-header",
   "metadata": {},
   "source": [
    "### Exercise 2 (Guided)\n",
    "\n",
    "**Problem:** Generate a random $4 \\times 4$ integer matrix with entries in $[-50, 50]$. Apply LLL. Then:\n",
    "\n",
    "1. Compute the orthogonality defect before and after reduction.\n",
    "2. Verify that all GSO coefficients $|\\mu_{i,j}| \\le 1/2$ in the reduced basis.\n",
    "3. Verify the Lovász condition holds for all consecutive pairs.\n",
    "\n",
    "**Hints:**\n",
    "- Use `random_matrix(ZZ, 4, 4, x=-50, y=51)` to generate the matrix.\n",
    "- Use the `check_lll_conditions()` function defined earlier.\n",
    "- Use the `basis_quality()` function for orthogonality defect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-exercise-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2. Fill in the blanks\n",
    "set_random_seed(7)  # for reproducibility\n",
    "\n",
    "# Step 1: Generate a random 4x4 matrix\n",
    "B = random_matrix(ZZ, 4, 4, x=-50, y=51)\n",
    "print('Random basis:')\n",
    "print(B)\n",
    "\n",
    "# Step 2: Apply LLL\n",
    "# L = ???\n",
    "\n",
    "# Step 3: Compute orthogonality defect before and after\n",
    "# Hint: basis_quality(B, 'Before') and basis_quality(L, 'After')\n",
    "\n",
    "# Step 4: Check LLL conditions on the reduced basis\n",
    "# Hint: check_lll_conditions(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-exercise-3-header",
   "metadata": {},
   "source": [
    "### Exercise 3 (Independent)\n",
    "\n",
    "**Problem:** Use LLL to find a small integer linear combination of the columns of:\n",
    "\n",
    "$$A = \\begin{pmatrix} 105 & 821 & 377 \\\\ 231 & 57 & 610 \\end{pmatrix}$$\n",
    "\n",
    "that gives a short vector. Specifically:\n",
    "\n",
    "1. Form the lattice generated by the columns of $A$ (i.e., $\\{A\\mathbf{x} : \\mathbf{x} \\in \\mathbb{Z}^3\\}$).\n",
    "2. Apply LLL to find a reduced basis.\n",
    "3. What is the shortest vector you find? What integer combination produces it?\n",
    "4. Can you beat LLL by trying random combinations? How many random attempts does it take to find something as short?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-exercise-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3. Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we explored the **LLL algorithm**. Key takeaways:\n",
    "\n",
    "- **Gram-Schmidt orthogonalization** provides the geometric backbone: it decomposes basis vectors into orthogonal components whose norms reveal the basis quality.\n",
    "- **LLL reduction** requires two conditions: size-reduction ($|\\mu_{i,j}| \\le 1/2$) and the Lovász condition ($\\|\\mathbf{b}_i^*\\|^2 \\ge (\\delta - \\mu_{i,i-1}^2)\\|\\mathbf{b}_{i-1}^*\\|^2$).\n",
    "- The algorithm alternates between **size-reducing** (cleaning up projections) and **swapping** (fixing Lovász violations), converging in polynomial time.\n",
    "- **Quality metrics** like orthogonality defect and Hermite factor quantify how \"nice\" a basis is.\n",
    "- **LLL guarantees** $\\|\\mathbf{b}_1\\| \\le 2^{(n-1)/2} \\cdot \\lambda_1(L)$ — polynomial time, but exponential approximation factor.\n",
    "- **Applications** include breaking knapsack crypto and finding integer relations — LLL is a universal tool for problems that can be cast as \"find a short vector.\"\n",
    "- **Limitations:** The approximation factor degrades exponentially with dimension, which is exactly what makes modern lattice-based cryptography possible.\n",
    "\n",
    "> **Looking ahead:** In notebook 08d, we introduce the **Learning With Errors (LWE)** problem — the computational hardness assumption behind Kyber/ML-KEM. The key insight: LWE problems live in dimensions large enough that even the best lattice reduction algorithms (LLL and its successors like BKZ) cannot find short enough vectors to break them.\n",
    "\n",
    "**Next:** [Learning With Errors](08d-learning-with-errors.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.5",
   "language": "sage",
   "name": "sagemath-10.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "sage",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
