{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.5",
   "language": "sage",
   "name": "sagemath-10.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "sage",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning With Errors\n",
    "\n",
    "**Module 08** | 08-lattices-post-quantum\n",
    "\n",
    "*LWE definition, noise, search vs decision*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "1. **Define** the Learning With Errors (LWE) problem and explain why adding noise to a linear system transforms it from trivially solvable to conjecturally hard.\n",
    "2. **Construct** LWE instances in SageMath and experimentally verify that Gaussian elimination fails in the presence of noise.\n",
    "3. **Distinguish** between Search-LWE and Decision-LWE, and explain their relationship.\n",
    "4. **Analyze** how the parameters $(n, q, \\sigma)$ control security, and **connect** LWE hardness to the lattice problems (SVP/CVP) studied in earlier notebooks.\n",
    "5. **Foreshadow** how LWE underpins Kyber (ML-KEM) and other post-quantum schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Completion of [The LLL Algorithm](08c-lll-algorithm.ipynb).\n",
    "- Familiarity with lattices, SVP/CVP, and the fact that LLL provides approximate solutions but cannot solve worst-case SVP/CVP exactly.\n",
    "- Comfort with matrices and vectors over $\\mathbb{Z}_q$ (modular arithmetic from Module 01)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Question\n",
    "\n",
    "> Solving a system of linear equations is easy --- Gaussian elimination does it in $O(n^3)$ time. What if I add a **tiny** bit of noise to every equation? Suddenly it becomes one of the hardest problems in mathematics.\n",
    "\n",
    "This is the central miracle of the Learning With Errors problem. A system $\\mathbf{b} = A\\mathbf{s}$ is trivial. A system $\\mathbf{b} = A\\mathbf{s} + \\mathbf{e}$ (where $\\mathbf{e}$ is \"small\") appears to be almost the same thing --- yet no known algorithm can solve it efficiently when the parameters are chosen correctly.\n",
    "\n",
    "**Bridge from 08c:** In the previous notebook, you saw that LLL can find short lattice vectors and approximately solve CVP. You might hope that LLL could strip away the noise in LWE. We will see that for *properly chosen* parameters, even LLL (and its stronger variants like BKZ) cannot recover the secret. This is precisely why LWE is the foundation of post-quantum cryptography."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Linear Systems Without Noise\n",
    "\n",
    "Let us start with something familiar. We pick a **secret** vector $\\mathbf{s} \\in \\mathbb{Z}_q^n$ and a random matrix $A \\in \\mathbb{Z}_q^{m \\times n}$, and compute $\\mathbf{b} = A\\mathbf{s} \\pmod{q}$.\n",
    "\n",
    "This is just a system of $m$ linear equations in $n$ unknowns over a finite field (when $q$ is prime). Gaussian elimination solves it instantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n = 6       # dimension (number of unknowns)\n",
    "m = 10      # number of equations (samples)\n",
    "q = 101     # modulus (prime)\n",
    "\n",
    "Zq = Zmod(q)\n",
    "\n",
    "# Secret vector\n",
    "set_random_seed(42)  # reproducibility\n",
    "s = random_vector(Zq, n)\n",
    "print(f'Secret s = {s}')\n",
    "print(f'Parameters: n={n}, m={m}, q={q}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random matrix A and compute b = A*s (NO noise)\n",
    "A = random_matrix(Zq, m, n)\n",
    "b_clean = A * s\n",
    "\n",
    "print('A (first 4 rows):')\n",
    "print(A[:4])\n",
    "print(f'\\nb = A*s = {b_clean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve using Gaussian elimination: A \\ b gives s\n",
    "# We use the first n rows (a square, invertible subsystem)\n",
    "A_square = A[:n]\n",
    "b_square = b_clean[:n]\n",
    "\n",
    "s_recovered = A_square.solve_right(b_square)\n",
    "print(f'Recovered s = {s_recovered}')\n",
    "print(f'Original  s = {s}')\n",
    "print(f'Match: {s_recovered == s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprise: a linear system over $\\mathbb{Z}_q$ is easy. Gaussian elimination recovers $\\mathbf{s}$ exactly.\n",
    "\n",
    "Now let us see what happens when we add noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The LWE Problem: Adding Noise\n",
    "\n",
    "**Definition (LWE).** Fix parameters $n$ (dimension), $q$ (modulus), and a noise distribution $\\chi$ (typically a discrete Gaussian with standard deviation $\\sigma$). The LWE problem is:\n",
    "\n",
    "$$\\text{Given } (A, \\mathbf{b}) \\text{ where } \\mathbf{b} = A\\mathbf{s} + \\mathbf{e} \\pmod{q},$$\n",
    "\n",
    "with $A \\xleftarrow{\\$} \\mathbb{Z}_q^{m \\times n}$, $\\mathbf{s} \\xleftarrow{\\$} \\mathbb{Z}_q^n$, and $\\mathbf{e} \\xleftarrow{} \\chi^m$ (each entry is small), **find** $\\mathbf{s}$.\n",
    "\n",
    "The key point: $\\mathbf{e}$ is *small* relative to $q$. Each entry of $\\mathbf{e}$ is typically in the range $[-\\sigma\\sqrt{2\\pi}, +\\sigma\\sqrt{2\\pi}]$ with high probability, while $q$ can be much larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate noise: small errors from a discrete Gaussian-like distribution\n",
    "# We use a simple rounded Gaussian for illustration\n",
    "from sage.stats.distributions.discrete_gaussian_integer import DiscreteGaussianDistributionIntegerSampler\n",
    "\n",
    "sigma = 3.0  # noise standard deviation\n",
    "D = DiscreteGaussianDistributionIntegerSampler(sigma=sigma)\n",
    "\n",
    "# Sample error vector\n",
    "e = vector(Zq, [D() for _ in range(m)])\n",
    "print(f'Error vector e = {e}')\n",
    "print(f'(Interpreting as signed: {[int(ei) if int(ei) < q/2 else int(ei)-q for ei in e]})')\n",
    "\n",
    "# Compute noisy b\n",
    "b_noisy = A * s + e\n",
    "print(f'\\nb_clean = A*s     = {b_clean}')\n",
    "print(f'b_noisy = A*s + e = {b_noisy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint\n",
    "\n",
    "**Predict before running the next cell:** If we apply Gaussian elimination to the noisy system $(A, \\mathbf{b}_{\\text{noisy}})$, will we recover $\\mathbf{s}$?\n",
    "\n",
    "Think about it: the system is $A\\mathbf{s} + \\mathbf{e} = \\mathbf{b}$, but Gaussian elimination \"thinks\" it is solving $A\\mathbf{x} = \\mathbf{b}$. It will find *some* solution, but will it be $\\mathbf{s}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to solve the NOISY system with Gaussian elimination\n",
    "A_square = A[:n]\n",
    "b_noisy_square = b_noisy[:n]\n",
    "\n",
    "s_attempt = A_square.solve_right(b_noisy_square)\n",
    "print(f'Gaussian elim gives: {s_attempt}')\n",
    "print(f'Actual secret s:     {s}')\n",
    "print(f'Match: {s_attempt == s}')\n",
    "print(f'\\nGaussian elimination FAILS! The noise has corrupted the solution.')\n",
    "print(f'The \"solution\" is a meaningless vector in Z_{q}^{n}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the core insight:\n",
    "\n",
    "| System | Difficulty |\n",
    "|--------|------------|\n",
    "| $\\mathbf{b} = A\\mathbf{s}$ | Trivial (Gaussian elimination) |\n",
    "| $\\mathbf{b} = A\\mathbf{s} + \\mathbf{e}$ | Conjectured hard (even for quantum computers) |\n",
    "\n",
    "A \"tiny\" perturbation transforms the problem from $O(n^3)$ to (conjectured) exponential.\n",
    "\n",
    "> **Misconception alert:** *\"LWE is just solving noisy equations, so just round to remove the noise.\"* Rounding works in very low dimensions (try it for $n=2$!), but in high dimensions the errors **accumulate** through the matrix operations. Gaussian elimination amplifies the noise catastrophically --- by the time you finish back-substitution, the errors have grown to fill all of $\\mathbb{Z}_q$, leaving you with a random vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the Noise\n",
    "\n",
    "Let us see what LWE \"looks like.\" We will generate many LWE samples and plot the residuals $\\mathbf{b} - A\\mathbf{s} \\pmod{q}$. Without noise, these are all zero. With noise, they form a cluster around zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate many LWE samples and visualize the noise\n",
    "M = 2000  # number of samples\n",
    "A_big = random_matrix(Zq, M, n)\n",
    "e_big = vector(Zq, [D() for _ in range(M)])\n",
    "b_big = A_big * s + e_big\n",
    "\n",
    "# Compute residuals: b - A*s (mod q)\n",
    "# If we knew s, these would reveal the noise pattern\n",
    "residuals = b_big - A_big * s\n",
    "\n",
    "# Convert to signed representation centered around 0\n",
    "def to_signed(x, q):\n",
    "    x = ZZ(x) % q\n",
    "    return x if x <= q//2 else x - q\n",
    "\n",
    "residuals_signed = [to_signed(r, q) for r in residuals]\n",
    "\n",
    "# Histogram of residuals\n",
    "p = histogram(residuals_signed, bins=range(-15, 16), color='steelblue',\n",
    "              edgecolor='white', density=True)\n",
    "p += text(f'Noise distribution (sigma={sigma})', (0, 0.15), fontsize=12, color='black')\n",
    "p.axes_labels(['Error value', 'Density'])\n",
    "show(p, figsize=(8, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compare: what does b look like vs A*s?\n",
    "# Plot the first component of b vs the first component of A*s\n",
    "As_vals = [ZZ((A_big * s)[i]) for i in range(min(500, M))]\n",
    "b_vals  = [ZZ(b_big[i]) for i in range(min(500, M))]\n",
    "\n",
    "p = scatter_plot(list(zip(As_vals, b_vals)), markersize=4, alpha=0.5,\n",
    "                 facecolor='steelblue')\n",
    "# Perfect line y=x for reference\n",
    "p += plot(x, (x, 0, q), color='red', linestyle='--', legend_label='b = As (no noise)')\n",
    "p.axes_labels(['A*s (mod q)', 'b = A*s + e (mod q)'])\n",
    "show(p, figsize=(6, 6), title='LWE samples cluster around the line y = x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot shows that each noisy observation $b_i$ is *close* to the true value $(A\\mathbf{s})_i$, but not exactly equal. The red dashed line is the noiseless case. The blue dots scatter around it --- that scatter IS the LWE noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision-LWE: Can You Tell Noise from Random?\n",
    "\n",
    "There is a second, equally important formulation of LWE:\n",
    "\n",
    "**Decision-LWE.** Given $(A, \\mathbf{b})$, distinguish between:\n",
    "- $\\mathbf{b} = A\\mathbf{s} + \\mathbf{e} \\pmod{q}$ (LWE samples), and\n",
    "- $\\mathbf{b} \\xleftarrow{\\$} \\mathbb{Z}_q^m$ (uniformly random).\n",
    "\n",
    "If the noise is large enough and $q$ is large enough, the LWE samples \"look random\" --- no efficient algorithm can tell them apart from uniform.\n",
    "\n",
    "Let us build a **distinguisher** and see when it works and when it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lwe_samples(n, m, q, sigma):\n",
    "    \"\"\"Generate an LWE instance: (A, b = A*s + e mod q).\"\"\"\n",
    "    Zq = Zmod(q)\n",
    "    D = DiscreteGaussianDistributionIntegerSampler(sigma=sigma)\n",
    "    s = random_vector(Zq, n)\n",
    "    A = random_matrix(Zq, m, n)\n",
    "    e = vector(Zq, [D() for _ in range(m)])\n",
    "    b = A * s + e\n",
    "    return A, b, s, e\n",
    "\n",
    "def generate_random_samples(n, m, q):\n",
    "    \"\"\"Generate uniform random (A, b) --- no LWE structure.\"\"\"\n",
    "    Zq = Zmod(q)\n",
    "    A = random_matrix(Zq, m, n)\n",
    "    b = random_vector(Zq, m)\n",
    "    return A, b\n",
    "\n",
    "print('Helper functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A naive distinguisher: try to solve A*x = b and check the residual\n",
    "# If it's LWE, the residual b - A*x_hat should be \"small\"\n",
    "# If it's random, the residual will be uniformly distributed\n",
    "\n",
    "def naive_distinguisher(A, b, q, threshold):\n",
    "    \"\"\"\n",
    "    Attempt to distinguish LWE from random.\n",
    "    Solve A*x = b over Z_q, compute residual, check if residual is 'small'.\n",
    "    Returns 'LWE' or 'Random'.\n",
    "    \"\"\"\n",
    "    n = A.ncols()\n",
    "    try:\n",
    "        # Use first n rows to solve\n",
    "        x_hat = A[:n].solve_right(b[:n])\n",
    "        # Check residual on ALL rows\n",
    "        residual = b - A * x_hat\n",
    "        # Convert to signed and compute average absolute value\n",
    "        res_signed = [abs(to_signed(ZZ(r), q)) for r in residual]\n",
    "        avg_residual = sum(res_signed) / len(res_signed)\n",
    "        return 'LWE' if avg_residual < threshold else 'Random'\n",
    "    except Exception:\n",
    "        return 'Unknown'\n",
    "\n",
    "print('Distinguisher defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the distinguisher with VERY SMALL noise (sigma=1)\n",
    "# It should work here because the noise is tiny relative to q\n",
    "print('=== Small noise (sigma=1, q=101) ===')\n",
    "print('The noise barely perturbs the system, so structure is detectable.\\n')\n",
    "\n",
    "correct = 0\n",
    "trials = 20\n",
    "for _ in range(trials):\n",
    "    # LWE instance\n",
    "    A_lwe, b_lwe, _, _ = generate_lwe_samples(n=6, m=20, q=101, sigma=1.0)\n",
    "    guess_lwe = naive_distinguisher(A_lwe, b_lwe, 101, threshold=q/4)\n",
    "    # Random instance\n",
    "    A_rand, b_rand = generate_random_samples(n=6, m=20, q=101)\n",
    "    guess_rand = naive_distinguisher(A_rand, b_rand, 101, threshold=q/4)\n",
    "    if guess_lwe == 'LWE':\n",
    "        correct += 1\n",
    "    if guess_rand == 'Random':\n",
    "        correct += 1\n",
    "\n",
    "print(f'Distinguisher accuracy: {correct}/{2*trials} = {100*correct/(2*trials):.0f}%')\n",
    "print('With tiny noise, the distinguisher works well!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try with PROPER noise (sigma large relative to q)\n",
    "# Use a larger q so sigma/q ratio is meaningful\n",
    "print('=== Proper noise (sigma=8, q=101) ===')\n",
    "print('Noise fills a significant fraction of Z_q. Can we still distinguish?\\n')\n",
    "\n",
    "correct = 0\n",
    "for _ in range(trials):\n",
    "    A_lwe, b_lwe, _, _ = generate_lwe_samples(n=6, m=20, q=101, sigma=8.0)\n",
    "    guess_lwe = naive_distinguisher(A_lwe, b_lwe, 101, threshold=q/4)\n",
    "    A_rand, b_rand = generate_random_samples(n=6, m=20, q=101)\n",
    "    guess_rand = naive_distinguisher(A_rand, b_rand, 101, threshold=q/4)\n",
    "    if guess_lwe == 'LWE':\n",
    "        correct += 1\n",
    "    if guess_rand == 'Random':\n",
    "        correct += 1\n",
    "\n",
    "print(f'Distinguisher accuracy: {correct}/{2*trials} = {100*correct/(2*trials):.0f}%')\n",
    "print('With larger noise, the distinguisher degrades toward random guessing (50%)!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaway:** When $\\sigma$ is small relative to $q$, the LWE distribution has detectable structure and a simple distinguisher works. When $\\sigma$ is chosen appropriately (large enough to mask the structure, but small enough that decryption still works), LWE samples become indistinguishable from random.\n",
    "\n",
    "This is the **Decision-LWE assumption**: for appropriate parameters, no polynomial-time algorithm can distinguish $(A, A\\mathbf{s} + \\mathbf{e})$ from $(A, \\mathbf{u})$ with non-negligible advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Search-LWE: Recovering the Secret\n",
    "\n",
    "**Search-LWE** asks: given $(A, \\mathbf{b} = A\\mathbf{s} + \\mathbf{e})$, find $\\mathbf{s}$.\n",
    "\n",
    "A classical result due to Regev (2005) shows that Search-LWE and Decision-LWE are **polynomially equivalent** when $q$ is polynomial in $n$. So if you can decide, you can search, and vice versa.\n",
    "\n",
    "Let us try brute force search in a tiny instance to see the structure, then observe how quickly it becomes infeasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute-force search for TINY parameters\n",
    "n_tiny, q_tiny, sigma_tiny = 3, 17, 1.5\n",
    "m_tiny = 8\n",
    "\n",
    "A_t, b_t, s_t, e_t = generate_lwe_samples(n_tiny, m_tiny, q_tiny, sigma_tiny)\n",
    "print(f'Tiny LWE: n={n_tiny}, q={q_tiny}, sigma={sigma_tiny}')\n",
    "print(f'Secret: s = {s_t}')\n",
    "print(f'Search space: q^n = {q_tiny}^{n_tiny} = {q_tiny^n_tiny} candidates\\n')\n",
    "\n",
    "# Try every possible s in Z_q^n\n",
    "Zq_tiny = Zmod(q_tiny)\n",
    "best_score = Infinity\n",
    "best_candidate = None\n",
    "\n",
    "for s_candidate in VectorSpace(Zq_tiny, n_tiny):\n",
    "    residual = b_t - A_t * s_candidate\n",
    "    # Score: sum of squared (signed) residuals\n",
    "    score = sum(to_signed(ZZ(r), q_tiny)^2 for r in residual)\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_candidate = s_candidate\n",
    "\n",
    "print(f'Best candidate: {best_candidate}  (score = {best_score})')\n",
    "print(f'Actual secret:  {s_t}')\n",
    "print(f'Match: {best_candidate == s_t}')\n",
    "print(f'\\nBrute force works for q^n = {q_tiny^n_tiny}, but real LWE uses n=512+, q~3329...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Connection to Lattices: LWE as CVP\n",
    "\n",
    "Why is LWE a \"lattice\" problem? Consider the lattice\n",
    "\n",
    "$$\\Lambda_q(A) = \\{\\mathbf{x} \\in \\mathbb{Z}^m : \\mathbf{x} \\equiv A\\mathbf{s} \\pmod{q} \\text{ for some } \\mathbf{s} \\in \\mathbb{Z}_q^n\\}.$$\n",
    "\n",
    "The vector $A\\mathbf{s} \\pmod{q}$ is a lattice point, and $\\mathbf{b} = A\\mathbf{s} + \\mathbf{e}$ is a point **near** the lattice (displaced by the small error $\\mathbf{e}$). So solving LWE is equivalent to solving a **Closest Vector Problem (CVP)** on this lattice: find the lattice point closest to $\\mathbf{b}$.\n",
    "\n",
    "We already know from 08b-08c that CVP is hard in general, and LLL only gives approximate solutions. LWE inherits this hardness.\n",
    "\n",
    "Let us verify this connection concretely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the q-ary lattice from A\n",
    "# Lattice Lambda_q(A) = { y in Z^m : y = A*s (mod q) for some s }\n",
    "# Basis: columns of [A^T | qI] give the lattice after transposing\n",
    "\n",
    "n_lat, m_lat, q_lat = 4, 8, 31\n",
    "sigma_lat = 2.0\n",
    "Zq_lat = Zmod(q_lat)\n",
    "\n",
    "A_lat, b_lat, s_lat, e_lat = generate_lwe_samples(n_lat, m_lat, q_lat, sigma_lat)\n",
    "\n",
    "# Construct the lattice basis (Kannan embedding style)\n",
    "# Rows of the basis matrix generate the lattice\n",
    "A_int = matrix(ZZ, A_lat)  # lift to integers\n",
    "\n",
    "# q-ary lattice basis: stack A^T on top of q*I\n",
    "basis_top = A_int.transpose()        # n x m\n",
    "basis_bot = q_lat * identity_matrix(ZZ, m_lat)  # m x m\n",
    "L_basis = block_matrix([[basis_top], [basis_bot]])  # (n+m) x m\n",
    "\n",
    "print(f'LWE instance: n={n_lat}, m={m_lat}, q={q_lat}')\n",
    "print(f'Secret s = {s_lat}')\n",
    "print(f'Error  e = {e_lat} (signed: {[to_signed(ZZ(ei), q_lat) for ei in e_lat]})')\n",
    "print(f'\\nLattice basis has {L_basis.nrows()} rows in Z^{L_basis.ncols()}')\n",
    "print(f'\\nThe target vector b is close to A*s in this lattice.')\n",
    "print(f'Distance (error norm) = {sqrt(sum(to_signed(ZZ(ei), q_lat)^2 for ei in e_lat)).n(digits=4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using LLL to approximately solve CVP (Babai's nearest plane)\n",
    "# For these small parameters, it might work!\n",
    "\n",
    "b_int = vector(ZZ, b_lat)\n",
    "\n",
    "# LLL-reduce the basis\n",
    "L_reduced = L_basis.LLL()\n",
    "\n",
    "# Babai's nearest plane algorithm\n",
    "def babai_cvp(basis, target):\n",
    "    \"\"\"Babai's nearest plane algorithm for approximate CVP.\"\"\"\n",
    "    B = basis.change_ring(QQ)\n",
    "    t = vector(QQ, target)\n",
    "    G, _ = B.gram_schmidt()\n",
    "    b = t\n",
    "    for i in range(B.nrows()-1, -1, -1):\n",
    "        c = (b * G[i]) / (G[i] * G[i])\n",
    "        b = b - round(c) * B[i]\n",
    "    return target - b  # closest lattice vector\n",
    "\n",
    "closest = babai_cvp(L_reduced, b_int)\n",
    "error_found = b_int - closest\n",
    "\n",
    "# The closest lattice vector should be A*s (mod q)\n",
    "As_int = vector(ZZ, A_lat * s_lat)\n",
    "\n",
    "print(f'Target b:                {b_int}')\n",
    "print(f'Closest lattice point:   {vector(ZZ(x) % q_lat for x in closest)}')\n",
    "print(f'True A*s mod q:          {As_int}')\n",
    "print(f'Recovered error:         {error_found}')\n",
    "print(f'True error (signed):     {[to_signed(ZZ(ei), q_lat) for ei in e_lat]}')\n",
    "print(f'\\nFor small parameters, LLL+Babai can crack LWE!')\n",
    "print(f'But as n grows and sigma/q is tuned, this approach fails.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Parameter Space: When Is LWE Hard?\n",
    "\n",
    "LWE security depends on three parameters:\n",
    "\n",
    "| Parameter | Role | Effect on security |\n",
    "|-----------|------|--------------------|\n",
    "| $n$ | Dimension | Larger $n$ = harder (exponential in $n$) |\n",
    "| $q$ | Modulus | Must be large enough for correctness, but not too large |\n",
    "| $\\sigma$ | Noise width | Larger noise = harder to solve, but too large breaks decryption |\n",
    "\n",
    "The critical ratio is $\\sigma / q$: if $\\sigma$ is too small relative to $q$, the noise is negligible and the system is easy. If $\\sigma$ is too large, decryption errors become likely.\n",
    "\n",
    "**Regev's reduction (2005):** Worst-case lattice problems (like GapSVP) reduce to average-case LWE when $\\sigma \\geq \\sqrt{n}$. This is why LWE is so compelling: breaking *any* LWE instance (even a random one) is as hard as solving *worst-case* lattice problems.\n",
    "\n",
    "Let us experimentally see how the dimension $n$ affects the difficulty of a lattice attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: try LLL-based attack for increasing n\n",
    "# For each n, generate LWE and see if Babai's algorithm recovers s\n",
    "\n",
    "print('n q sigma LLL recovers s? time (s)')",
    "\n",
    "\n",
    "for n_exp in [4, 6, 8, 12, 16, 20]:\n",
    "    q_exp = next_prime(n_exp^2 * 10)  # q ~ O(n^2)\n",
    "    sigma_exp = max(2.0, sqrt(n_exp))  # sigma ~ sqrt(n)\n",
    "    m_exp = 2 * n_exp\n",
    "    Zq_exp = Zmod(q_exp)\n",
    "\n",
    "    A_exp, b_exp, s_exp, e_exp = generate_lwe_samples(n_exp, m_exp, q_exp, float(sigma_exp))\n",
    "\n",
    "    # Build lattice and try LLL + Babai\n",
    "    A_exp_int = matrix(ZZ, A_exp)\n",
    "    basis_exp = block_matrix([[A_exp_int.transpose()], [q_exp * identity_matrix(ZZ, m_exp)]])\n",
    "\n",
    "    t0 = walltime()\n",
    "    try:\n",
    "        L_red = basis_exp.LLL()\n",
    "        closest_exp = babai_cvp(L_red, vector(ZZ, b_exp))\n",
    "        # Check: does closest mod q equal A*s mod q?\n",
    "        closest_mod = vector(Zq_exp, [ZZ(x) % q_exp for x in closest_exp])\n",
    "        As_mod = A_exp * s_exp\n",
    "        success = (closest_mod == As_mod)\n",
    "    except Exception:\n",
    "        success = False\n",
    "    elapsed = walltime() - t0\n",
    "\n",
    "    print(f'{n_exp} {q_exp} {float(sigma_exp):>6.1f} {str(success)} {elapsed:>10.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $n$ increases, the LLL-based attack starts to fail. For the parameters used in real-world schemes (like Kyber, where $n = 256$ per polynomial), the lattice dimension is in the hundreds or thousands, and no known algorithm --- classical or quantum --- can solve LWE.\n",
    "\n",
    "> **Crypto foreshadowing:** Kyber (ML-KEM), the NIST-selected post-quantum key encapsulation mechanism, is built on **Module-LWE** --- a structured variant of LWE where the matrix $A$ has a special block structure using polynomial rings. The noise $\\mathbf{e}$ IS the security: without it, Kyber would be trivially breakable by linear algebra. The next notebooks (08e, 08f) show how the ring structure makes this efficient enough for real-world use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Noise Threshold for Gaussian Elimination (Worked)\n",
    "\n",
    "**Goal:** Experimentally find the noise level at which Gaussian elimination stops recovering the secret.\n",
    "\n",
    "**Setup:** Fix $n = 5$, $q = 101$, $m = 10$. For increasing $\\sigma$ values, generate LWE instances and try to solve with Gaussian elimination. Measure the fraction of correct recoveries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 1 - FULLY WORKED SOLUTION\n",
    "\n",
    "n_ex, q_ex, m_ex = 5, 101, 10\n",
    "Zq_ex = Zmod(q_ex)\n",
    "num_trials = 50\n",
    "\n",
    "sigma_values = [0.0, 0.5, 1.0, 2.0, 3.0, 5.0, 8.0, 12.0]\n",
    "success_rates = []\n",
    "\n",
    "for sig in sigma_values:\n",
    "    successes = 0\n",
    "    for _ in range(num_trials):\n",
    "        s_ex = random_vector(Zq_ex, n_ex)\n",
    "        A_ex = random_matrix(Zq_ex, m_ex, n_ex)\n",
    "\n",
    "        if sig == 0:\n",
    "            e_ex = zero_vector(Zq_ex, m_ex)\n",
    "        else:\n",
    "            D_ex = DiscreteGaussianDistributionIntegerSampler(sigma=float(sig))\n",
    "            e_ex = vector(Zq_ex, [D_ex() for _ in range(m_ex)])\n",
    "\n",
    "        b_ex = A_ex * s_ex + e_ex\n",
    "\n",
    "        # Try Gaussian elimination on first n rows\n",
    "        try:\n",
    "            s_guess = A_ex[:n_ex].solve_right(b_ex[:n_ex])\n",
    "            if s_guess == s_ex:\n",
    "                successes += 1\n",
    "        except Exception:\n",
    "            pass  # singular matrix\n",
    "\n",
    "    rate = successes / num_trials\n",
    "    success_rates.append(rate)\n",
    "    print(f'sigma = {sig:>5.1f}: success rate = {rate:.0%}')\n",
    "\n",
    "# Plot\n",
    "p = list_plot(list(zip(sigma_values, success_rates)), plotjoined=True,\n",
    "              marker='o', color='steelblue', thickness=2)\n",
    "p.axes_labels(['Noise sigma', 'Gaussian elim. success rate'])\n",
    "show(p, figsize=(7, 4), title=f'n={n_ex}, q={q_ex}: noise kills Gaussian elimination')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** At $\\sigma = 0$, Gaussian elimination always succeeds. By $\\sigma \\geq 1$, the success rate drops sharply. Even tiny noise (relative to $q = 101$) is enough to destroy exact linear algebra. This is the fundamental principle behind LWE security."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build a Better Distinguisher (Guided)\n",
    "\n",
    "The naive distinguisher from Section 4 uses average residual magnitude. A better approach uses the **chi-squared statistic**: if the residuals are LWE noise, they cluster near zero; if random, they are uniform over $\\mathbb{Z}_q$.\n",
    "\n",
    "**Task:** Complete the function below to implement a chi-squared distinguisher.\n",
    "\n",
    "*Hints:*\n",
    "1. Compute residuals $\\mathbf{b} - A\\hat{\\mathbf{s}} \\pmod{q}$ where $\\hat{\\mathbf{s}}$ is the Gaussian elimination \"solution.\"\n",
    "2. Bin the signed residuals into a histogram.\n",
    "3. Compare against the uniform distribution using chi-squared: $\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}$.\n",
    "4. A large $\\chi^2$ means the distribution is NOT uniform, suggesting LWE structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 2 - GUIDED (fill in the marked sections)\n",
    "\n",
    "def chi_squared_distinguisher(A, b, q, chi2_threshold):\n",
    "    \"\"\"\n",
    "    Distinguisher based on chi-squared test of residuals.\n",
    "    Returns 'LWE' if residuals show non-uniform structure, else 'Random'.\n",
    "    \"\"\"\n",
    "    n = A.ncols()\n",
    "    m = A.nrows()\n",
    "    Zq = Zmod(q)\n",
    "\n",
    "    # Step 1: Solve A[:n]*x = b[:n] to get candidate x_hat\n",
    "    try:\n",
    "        x_hat = A[:n].solve_right(b[:n])\n",
    "    except Exception:\n",
    "        return 'Unknown'\n",
    "\n",
    "    # Step 2: Compute residuals on ALL m equations\n",
    "    residual = b - A * x_hat\n",
    "    res_signed = [to_signed(ZZ(r), q) for r in residual]\n",
    "\n",
    "    # Step 3: Build histogram of residuals\n",
    "    # ---- FILL IN ----\n",
    "    # Count how many residuals fall in each bin from -(q//2) to +(q//2)\n",
    "    # Then compute chi-squared against the uniform expectation (m/q per bin)\n",
    "    #\n",
    "    # counts = {}  # bin -> count\n",
    "    # for r in res_signed:\n",
    "    #     ...\n",
    "    # expected = m / q  # uniform expectation per bin\n",
    "    # chi2 = sum((count - expected)^2 / expected for count in counts.values())\n",
    "    # ---- END FILL IN ----\n",
    "\n",
    "    # Step 4: Large chi2 means non-uniform => LWE\n",
    "    # return 'LWE' if chi2 > chi2_threshold else 'Random'\n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# Test your distinguisher:\n",
    "# A_test, b_test, _, _ = generate_lwe_samples(6, 30, 101, 2.0)\n",
    "# print(chi_squared_distinguisher(A_test, b_test, 101, chi2_threshold=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: LWE Parameter Exploration (Independent)\n",
    "\n",
    "**Task:** Write a complete experiment that explores the \"security frontier\" of LWE.\n",
    "\n",
    "1. For $n \\in \\{4, 8, 12, 16\\}$ and $\\sigma \\in \\{1, 2, 4, 8\\}$, with $q$ the smallest prime $\\geq 4n\\sigma$:\n",
    "   - Generate 20 LWE instances.\n",
    "   - For each, attempt to recover $\\mathbf{s}$ using the LLL + Babai approach from Section 6.\n",
    "   - Record the success rate.\n",
    "\n",
    "2. Produce a heatmap (or table) showing success rate as a function of $(n, \\sigma)$.\n",
    "\n",
    "3. **Answer these questions:**\n",
    "   - At what point does the LLL attack stop working?\n",
    "   - How does this relate to the ratio $\\sigma / q$?\n",
    "   - What is the minimum $n$ for which the attack never succeeds (for any $\\sigma$ tested)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 3 - INDEPENDENT\n",
    "# Write your solution here.\n",
    "#\n",
    "# Skeleton:\n",
    "# results = {}\n",
    "# for n_val in [4, 8, 12, 16]:\n",
    "#     for sigma_val in [1, 2, 4, 8]:\n",
    "#         q_val = next_prime(4 * n_val * sigma_val)\n",
    "#         ...\n",
    "#         results[(n_val, sigma_val)] = success_rate\n",
    "#\n",
    "# Print or plot results as a heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we explored **Learning With Errors (LWE)**, the computational problem at the heart of post-quantum cryptography. Key takeaways:\n",
    "\n",
    "- **Without noise**, $\\mathbf{b} = A\\mathbf{s}$ is trivially solvable by Gaussian elimination. **With noise**, $\\mathbf{b} = A\\mathbf{s} + \\mathbf{e}$ becomes conjectured hard --- even for quantum computers.\n",
    "\n",
    "- **Decision-LWE** (distinguishing LWE from random) and **Search-LWE** (finding $\\mathbf{s}$) are polynomially equivalent. Both are as hard as worst-case lattice problems (GapSVP, SIVP) via Regev's reduction.\n",
    "\n",
    "- **LWE is a lattice problem in disguise:** solving LWE corresponds to finding the closest lattice point (CVP) in a $q$-ary lattice. LLL-based attacks work for toy parameters but fail as $n$ grows.\n",
    "\n",
    "- **Parameters matter:** the triple $(n, q, \\sigma)$ must be balanced. Too little noise: insecure. Too much noise: decryption fails. The sweet spot gives us both security and correctness.\n",
    "\n",
    "- **Post-quantum schemes** like Kyber (ML-KEM) are built on structured variants of LWE. The noise is not a bug --- it IS the security.\n",
    "\n",
    "**Next:** [Ring-LWE](08e-ring-lwe.ipynb) --- where we add algebraic structure to make LWE practical."
   ]
  }
 ]
}
